{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WriteRight: An English dialect translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**: Communication is challenging when people speak different dialects. In a business setting, this can result in lost time, sales, and ultimately revenue. Communication is such an extensive problem, that it is estimated ~$40B will be spent on translation services in 2018. Currently, machine learning based translation services focus on translations between different languages, thus there is a need for dialect translation services.\n",
    "\n",
    "Here, I developed an English dialect translator to translate between American and British English. Feel free to explore the translator at [http://www.writeright.cc](http://www.writeright.cc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis details below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recently added large corpora of sentences to a Postgresql database. I want to use these data as new training data to see if we can do a better job of translating. Each corpus has millions of words and are specific to American and British dialects. Keep your fingers crossed..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets see what we have in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from gensim import models, utils\n",
    "from gensim.models import translation_matrix\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a database name\n",
    "# Set your postgres username\n",
    "dbname = 'corpus'\n",
    "username = 'dan' # change this to your username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgres://dan@localhost/corpus\n"
     ]
    }
   ],
   "source": [
    "## 'engine' is a connection to a database\n",
    "## Here, we're using postgres, but sqlalchemy can connect to other things too.\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgres://%s@localhost/%s'%(username,dbname))\n",
    "print(engine.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = None\n",
    "conn = psycopg2.connect(database = dbname, user = username)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# create table one by one\n",
    "\n",
    "# close communication with the PostgreSQL database server\n",
    "#cur.close()\n",
    "# commit the changes\n",
    "#conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('training_data',)]\n"
     ]
    }
   ],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute(\"select relname from pg_class where relkind='r' and relname !~ '^(pg_|sql_)';\")\n",
    "print (cur.fetchall())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data into Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = '''SELECT * \n",
    "             FROM training_data;\n",
    "             '''\n",
    "\n",
    "# Read in the data into Pandas\n",
    "df = pd.read_sql_query(command, con=engine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BNC     12736688\n",
       "OANC      627803\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.source.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_dup = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to change 'BNC' and 'OANC' to number values... Sadly, I didn't comment this earlier, and I don't remember why :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dan/anaconda3/envs/insight/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "temp_replace = {'BNC':0, 'OANC': 1}\n",
    "df_non_dup['source'] = df_non_dup['source'].replace(temp_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>have  you  done  much  work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>i've  done  some  work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>i've  tried</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>to  sort  of</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source                      sentence\n",
       "0       0  have  you  done  much  work \n",
       "1       0       i've  done  some  work \n",
       "2       0                 i've  tried  \n",
       "3       0                         good \n",
       "4       0                to  sort  of  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_non_dup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the poor data\n",
    "\n",
    "def standardize_text(temp_df, text_field):\n",
    "    temp_df[text_field] = temp_df[text_field].str.replace(r\"http\\S+\", \"\")\n",
    "    temp_df[text_field] = temp_df[text_field].str.replace(r\"http\", \"\")\n",
    "    temp_df[text_field] = temp_df[text_field].str.replace(r\"@\\S+\", \"\")\n",
    "    temp_df[text_field] = temp_df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\`\\\"\\_\\n]\", \" \")\n",
    "    temp_df[text_field] = temp_df[text_field].str.replace(r\"@\", \"at\")\n",
    "    temp_df[text_field] = temp_df[text_field].str.lower()\n",
    "    return temp_df\n",
    "\n",
    "#df_non_dup = standardize_text(df_non_dup, \"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>have  you  done  much  work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>i've  done  some  work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>i've  tried</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>to  sort  of</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source                      sentence\n",
       "0       0  have  you  done  much  work \n",
       "1       0       i've  done  some  work \n",
       "2       0                 i've  tried  \n",
       "3       0                         good \n",
       "4       0                to  sort  of  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_non_dup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dan/anaconda3/envs/insight/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>have  you  done  much  work</td>\n",
       "      <td>[have, you, done, much, work]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>i've  done  some  work</td>\n",
       "      <td>[i, ve, done, some, work]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>i've  tried</td>\n",
       "      <td>[i, ve, tried]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>good</td>\n",
       "      <td>[good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>to  sort  of</td>\n",
       "      <td>[to, sort, of]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source                      sentence                         tokens\n",
       "0       0  have  you  done  much  work   [have, you, done, much, work]\n",
       "1       0       i've  done  some  work       [i, ve, done, some, work]\n",
       "2       0                 i've  tried                   [i, ve, tried]\n",
       "3       0                         good                          [good]\n",
       "4       0                to  sort  of                   [to, sort, of]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize the sentences\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "df_non_dup[\"tokens\"] = df_non_dup[\"sentence\"].apply(tokenizer.tokenize)\n",
    "df_non_dup.head()\n",
    "\n",
    "\n",
    "# I'm using the TweetTokenizer because it doesn't split contractions\n",
    "\n",
    "#from nltk.tokenize import TweetTokenizer\n",
    "#tokenizer = TweetTokenizer()\n",
    "#df_non_dup[\"tokens\"] = df_non_dup[\"sentence\"].apply(tokenizer.tokenize)\n",
    "#df_non_dup.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The phrasing as implemented here doesn't work unless an individual inputs the new word as 'first_second', which would never happen. So, I've commented out these sections and will re-run it without it.\n",
    "Identify phrases that exist in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#british_phrases = models.phrases.Phrases(df_non_dup.loc[df_non_dup['source'] == 0, 'tokens'])\n",
    "#american_phrases = models.phrases.Phrases(df_non_dup.loc[df_non_dup['source'] == 1, 'tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the tokens with our newly identified phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_non_dup.loc[df_non_dup['source']==0, 'tokens'].replace(british_phrases[df_non_dup.loc[df_non_dup['source']==0, 'tokens']], inplace=True)\n",
    "#df_non_dup.loc[df_non_dup['source']==1, 'tokens'].replace(american_phrases[df_non_dup.loc[df_non_dup['source']==1, 'tokens']], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                               [have, you, done, much, work]\n",
       "1                                   [i, ve, done, some, work]\n",
       "2                                              [i, ve, tried]\n",
       "3                                                      [good]\n",
       "4                                              [to, sort, of]\n",
       "6           [what, the, hell, i, was, supposed, to, do, with]\n",
       "7                                    [what, were, you, doing]\n",
       "8           [there, s, some, i, m, totally, totally, confu...\n",
       "9                         [you, were, doing, differentiating]\n",
       "10                                                [and, then]\n",
       "11                                                     [yeah]\n",
       "12          [leaving, it, and, then, going, back, to, it, ...\n",
       "13                                             [some, of, it]\n",
       "15          [no, no, i, don, t, know, what, i, ve, done, w...\n",
       "16               [i, ve, been, concentrating, on, trying, to]\n",
       "17                                                     [okay]\n",
       "18          [i, ve, gone, through, these, i, don, t, know,...\n",
       "19                                                    [right]\n",
       "20          [i, done, them, and, then, i, ve, took, them, ...\n",
       "21          [i, ve, g, i, ve, got, about, a, half, a, doze...\n",
       "22          [and, i, ve, decided, to, do, it, in, this, co...\n",
       "24                                    [so, i, ve, done, that]\n",
       "25                            [now, i, can, understand, that]\n",
       "26                                         [understand, that]\n",
       "28                                     [that, goes, to, that]\n",
       "29                                                   [two, x]\n",
       "30                                [but, i, m, not, sure, why]\n",
       "31                    [now, is, that, that, cosh, squared, x]\n",
       "34                          [and, that, is, goes, to, two, x]\n",
       "35                                        [shine, squared, x]\n",
       "                                  ...                        \n",
       "13364461    [in, the, period, between, december, 1999, and...\n",
       "13364462    [the, flow, from, the, fbi, was, particularly,...\n",
       "13364463    [that, from, the, intelligence, community, was...\n",
       "13364464    [and, the, terrorist, threat, in, the, united,...\n",
       "13364465                                 [why, was, this, so]\n",
       "13364466    [most, obviously, it, was, because, everyone, ...\n",
       "13364467    [then, jordanian, authorities, arrested, 16, a...\n",
       "13364468    [those, in, custody, included, two, u, s, citi...\n",
       "13364469    [soon, after, an, alert, customs, agent, caugh...\n",
       "13364470    [he, was, found, to, have, confederates, on, b...\n",
       "13364471    [these, were, not, events, whispered, about, i...\n",
       "13364472    [the, information, was, in, all, major, newspa...\n",
       "13364473    [though, the, jordanian, arrests, only, made, ...\n",
       "13364474    [the, arrest, of, ressam, was, on, front, page...\n",
       "13364475    [fbi, field, offices, around, the, country, we...\n",
       "13364476    [representatives, of, the, justice, department...\n",
       "13364477    [after, the, millennium, alert, the, governmen...\n",
       "13364478    [counterterrorism, went, back, to, being, a, s...\n",
       "13364479    [but, the, experience, showed, that, the, gove...\n",
       "13364480    [while, one, factor, was, the, preexistence, o...\n",
       "13364481    [everyone, knew, not, only, of, an, abstract, ...\n",
       "13364482    [terrorism, had, a, face, that, of, ahmed, res...\n",
       "13364483    [in, the, summer, of, 2001, dci, tenet, the, c...\n",
       "13364484    [but, the, millennium, phenomenon, was, not, r...\n",
       "13364485    [fbi, field, offices, apparently, saw, no, abn...\n",
       "13364486    [between, may, 2001, and, september, 11, there...\n",
       "13364487    [front, page, stories, touching, on, the, subj...\n",
       "13364488    [all, this, reportage, looked, backward, descr...\n",
       "13364489    [back, page, notices, told, of, tightened, sec...\n",
       "13364490                        [all, the, rest, was, secret]\n",
       "Name: tokens, Length: 10298943, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_non_dup.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect our data a bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11802101 words total, with a vocabulary size of 161227\n",
      "Max sentence length is 843\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#df.loc[df['B'] == 3, 'A']\n",
    "#all_words = [word for tokens in df_non_dup.loc[df_non_dup['source']==1, 'tokens'] for word in tokens]\n",
    "all_words = [word for tokens in df_non_dup[\"tokens\"] for word in tokens]\n",
    "sentence_lengths = [len(tokens) for tokens in df_non_dup[\"tokens\"]]\n",
    "VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  2.26321100e+06,   3.70849800e+06,   2.11640200e+06,\n",
       "          1.11177700e+06,   5.59508000e+05,   2.73699000e+05,\n",
       "          1.32289000e+05,   6.39140000e+04,   3.16900000e+04,\n",
       "          1.63790000e+04,   8.62600000e+03,   4.74000000e+03,\n",
       "          2.75400000e+03,   1.70400000e+03,   1.04400000e+03,\n",
       "          7.21000000e+02,   4.66000000e+02,   3.10000000e+02,\n",
       "          2.69000000e+02]),\n",
       " array([ 0,  5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80,\n",
       "        85, 90, 95]),\n",
       " <a list of 19 Patch objects>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAD8CAYAAACyyUlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrP+SFZ21WQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cc75e1860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sentence_lengths, bins=range(0,100,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "british_vec = gensim.models.Word2Vec(df_non_dup.loc[df_non_dup['source'] == 0, 'tokens'], size=300)\n",
    "american_vec = gensim.models.Word2Vec(df_non_dup.loc[df_non_dup['source'] == 1, 'tokens'], size=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#british_vec.wv.save(\"british_big\")\n",
    "#american_vec.wv.save(\"american_big\")\n",
    "british_vec = gensim.models.KeyedVectors.load('british_big')\n",
    "american_vec = gensim.models.KeyedVectors.load('american_big')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create word pairs that will serve as the \"map\" to connect the British and American vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file = \"training_words1000.txt\" # from https://www.ef.edu/english-resources/english-vocabulary/top-1000-words/\n",
    "\n",
    "with utils.smart_open(train_file, \"r\") as f:\n",
    "    word_pair = [tuple(utils.any2unicode(line.lower()).strip().split()) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transmat_us_to_uk = models.translation_matrix.TranslationMatrix(american_vec.wv, british_vec.wv)\n",
    "transmat_us_to_uk.train(word_pair)\n",
    "transmat_uk_to_us = models.translation_matrix.TranslationMatrix(british_vec.wv, american_vec.wv)\n",
    "transmat_uk_to_us.train(word_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to \"validate\" my model using text from Harry Potter and the Half-Blood Prince."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uk_words = []\n",
    "us_words = []\n",
    "\n",
    "#with utils.smart_open(train_file, \"r\") as f:\n",
    "#    word_pair = [tuple(utils.any2unicode(line.lower()).strip().split()) for line in f]\n",
    "header = True    \n",
    "with open('harry_potter_half_blood_prince_comparisons.txt', 'r') as hp: # uk words are in the first column and us in second\n",
    "    for line in hp:\n",
    "        if header:\n",
    "            header = False\n",
    "        else:\n",
    "            tmp_words = line.lower().split()\n",
    "            uk_words.append(tmp_words[0])\n",
    "            us_words.append(tmp_words[1])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test American to British"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_translation(original_words, translated_words, source_vec, destination_vec, number_to_return):\n",
    "    correct = 0\n",
    "    not_in_model = 0\n",
    "    tmp_correct_index = []\n",
    "    tmp_incorrect_index = []\n",
    "    \n",
    "    transmat = models.translation_matrix.TranslationMatrix(source_vec.wv, destination_vec.wv)\n",
    "    transmat.train(word_pair)\n",
    "\n",
    "    for i in range(len(original_words)): # using 1 instead of 0 because the list has a header\n",
    "        try:\n",
    "            output_dict = transmat.translate(original_words[i], topn=number_to_return, source_lang_vec=source_vec.wv, target_lang_vec=destination_vec.wv)\n",
    "            #print(translated_words[i], output_dict[original_words[i]])\n",
    "            if translated_words[i] in output_dict[original_words[i]]:\n",
    "                correct += 1\n",
    "                tmp_correct_index.append(i)\n",
    "            else:\n",
    "                tmp_incorrect_index.append(i)\n",
    "        except:\n",
    "            not_in_model += 1\n",
    "    return([correct/len(original_words), not_in_model/len(original_words), tmp_correct_index, tmp_incorrect_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test US to UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35353535353535354, 0.09090909090909091, [0, 4, 5, 6, 9, 11, 12, 14, 17, 19, 25, 29, 31, 33, 34, 35, 41, 44, 47, 49, 51, 52, 58, 62, 64, 67, 70, 72, 74, 81, 83, 84, 91, 94, 98], [1, 2, 3, 7, 8, 10, 15, 16, 18, 20, 23, 24, 26, 27, 30, 32, 36, 37, 38, 40, 42, 43, 45, 46, 48, 53, 54, 55, 56, 59, 60, 61, 63, 65, 66, 68, 69, 71, 73, 75, 76, 77, 79, 80, 82, 85, 86, 87, 88, 89, 90, 92, 93, 95, 97]]\n",
      "[0.43434343434343436, 0.09090909090909091, [0, 3, 4, 5, 6, 9, 11, 12, 14, 17, 19, 25, 29, 31, 33, 34, 35, 41, 44, 47, 49, 51, 52, 53, 56, 58, 62, 64, 65, 67, 69, 70, 72, 74, 81, 83, 84, 85, 89, 90, 91, 94, 98], [1, 2, 7, 8, 10, 15, 16, 18, 20, 23, 24, 26, 27, 30, 32, 36, 37, 38, 40, 42, 43, 45, 46, 48, 54, 55, 59, 60, 61, 63, 66, 68, 71, 73, 75, 76, 77, 79, 80, 82, 86, 87, 88, 92, 93, 95, 97]]\n"
     ]
    }
   ],
   "source": [
    "# this prints the \n",
    "print(test_translation(us_words, uk_words, american_vec, british_vec, 5))\n",
    "print(test_translation(us_words, uk_words, american_vec, british_vec, 10))\n",
    "us_to_uk_correct, us_to_uk_missing, us_to_uk_correct_index, us_to_uk_incorrect_index = test_translation(us_words, uk_words, american_vec, british_vec, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**US to UK translation results:**\n",
    "When checking to see if the expected word was in the top **5 translations was identified 35%** and **43% when that was expanded to the top 10**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test UK to US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1919191919191919, 0.050505050505050504, [2, 3, 4, 6, 7, 8, 10, 17, 47, 49, 51, 52, 70, 72, 83, 84, 90, 97, 98], [0, 1, 5, 9, 11, 12, 14, 15, 18, 19, 20, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96]]\n",
      "[0.2828282828282828, 0.050505050505050504, [0, 2, 3, 4, 6, 7, 8, 10, 14, 17, 19, 25, 29, 31, 47, 49, 51, 52, 62, 70, 72, 73, 79, 83, 84, 90, 97, 98], [1, 5, 9, 11, 12, 15, 18, 20, 22, 23, 24, 26, 27, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 71, 74, 75, 76, 77, 78, 80, 81, 82, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96]]\n"
     ]
    }
   ],
   "source": [
    "print(test_translation(uk_words, us_words, british_vec, american_vec, 5))\n",
    "print(test_translation(uk_words, us_words, british_vec, american_vec, 10))\n",
    "uk_to_us_correct, uk_to_us_missing, uk_to_us_correct_index, uk_to_us_incorrect_index = test_translation(uk_words, us_words, british_vec, american_vec, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UK to US translation results:**\n",
    "When checking to see if the expected word was in the top **5 translations was identified 19%** and **28% when that was expanded to the top 10**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count how many times each word was observed in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a function that counts the number of occurrences of a word in the 'tokens' column\n",
    "def count_word_occurrences(tmp_word, tmp_source):\n",
    "    total_count = 0\n",
    "    tmp_df = df_non_dup.loc[df_non_dup['source'] == tmp_source]\n",
    "    for sentence in tmp_df['sentence']:\n",
    "        total_count += sentence.count(tmp_word)\n",
    "    return(total_count)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many times is each word observed in the data for the words we could translate and the words that we couldn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "incorrect_words_count_us_to_uk = []\n",
    "for i in us_to_uk_correct_index:\n",
    "    incorrect_words_count_us_to_uk.append([us_words[i], count_word_occurrences(us_words[i], 0), uk_words[i], count_word_occurrences(uk_words[i], 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_words_count_us_to_uk = []\n",
    "for i in us_to_uk_incorrect_index:\n",
    "    correct_words_count_us_to_uk.append([us_words[i], count_word_occurrences(us_words[i], 0), uk_words[i], count_word_occurrences(uk_words[i], 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_of_good_translations = pd.DataFrame(correct_words_count_us_to_uk)\n",
    "count_of_good_translations.columns = ['us_word', 'us_count', 'uk_word', 'uk_count']\n",
    "count_of_good_translations['translated'] = 0\n",
    "\n",
    "count_of_bad_translations = pd.DataFrame(incorrect_words_count_us_to_uk)\n",
    "count_of_bad_translations.columns = ['us_word', 'us_count', 'uk_word', 'uk_count']\n",
    "count_of_bad_translations['translated'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_of_good_and_bad_translations.boxplot(column='us_count', by='translated')\n",
    "plt.ylim(ymax=1000000)\n",
    "plt.ylim(ymin=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>us_count</th>\n",
       "      <th>uk_count</th>\n",
       "      <th>translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.800000e+01</td>\n",
       "      <td>4.800000e+01</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.414534e+05</td>\n",
       "      <td>7.623219e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.921544e+06</td>\n",
       "      <td>2.148370e+05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.090000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.467500e+02</td>\n",
       "      <td>4.425000e+01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.149500e+03</td>\n",
       "      <td>1.850000e+03</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.454058e+05</td>\n",
       "      <td>2.633100e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.897423e+06</td>\n",
       "      <td>1.132578e+06</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           us_count      uk_count  translated\n",
       "count  4.800000e+01  4.800000e+01        48.0\n",
       "mean   8.414534e+05  7.623219e+04         0.0\n",
       "std    1.921544e+06  2.148370e+05         0.0\n",
       "min    1.090000e+02  0.000000e+00         0.0\n",
       "25%    9.467500e+02  4.425000e+01         0.0\n",
       "50%    8.149500e+03  1.850000e+03         0.0\n",
       "75%    1.454058e+05  2.633100e+04         0.0\n",
       "max    7.897423e+06  1.132578e+06         0.0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_of_good_and_bad_translations.loc[count_of_good_and_bad_translations['translated']==0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>us_count</th>\n",
       "      <th>uk_count</th>\n",
       "      <th>translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.200000e+01</td>\n",
       "      <td>4.200000e+01</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.327274e+06</td>\n",
       "      <td>2.235158e+05</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.180410e+06</td>\n",
       "      <td>3.852402e+05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.050000e+02</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.335550e+04</td>\n",
       "      <td>1.132275e+04</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.263320e+05</td>\n",
       "      <td>4.630000e+04</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.700210e+05</td>\n",
       "      <td>1.385950e+05</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.194780e+06</td>\n",
       "      <td>1.132578e+06</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           us_count      uk_count  translated\n",
       "count  4.200000e+01  4.200000e+01        42.0\n",
       "mean   1.327274e+06  2.235158e+05         1.0\n",
       "std    2.180410e+06  3.852402e+05         0.0\n",
       "min    1.050000e+02  2.000000e+00         1.0\n",
       "25%    5.335550e+04  1.132275e+04         1.0\n",
       "50%    5.263320e+05  4.630000e+04         1.0\n",
       "75%    8.700210e+05  1.385950e+05         1.0\n",
       "max    8.194780e+06  1.132578e+06         1.0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_of_good_and_bad_translations.loc[count_of_good_and_bad_translations['translated']==1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_of_non_translated_words_uk_to_us = df_non_dup.tokens.value_counts()[uk_to_us_incorrect_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the words that were not translated properly had a low number of occurrences in the training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
